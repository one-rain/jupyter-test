{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efa4b62",
   "metadata": {},
   "source": [
    "Embedding就是利用Embedding模型将本地知识转为向量的过程。\n",
    "下面的例子将简单的介绍怎么使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "'''\n",
    "Embedding案例\n",
    "'''\n",
    "\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    "    base_url = 'http://localhost:11434'\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 嵌入单例文本\n",
    "input_text = \"2025年9月3日，中国将在北京举行抗战胜利80周年纪念活动。\"\n",
    "vector = embed.embed_query(input_text)\n",
    "# 异步方式\n",
    "# vector = await embed.aembed_query(input_text)\n",
    "print(vector[:3])\n",
    "\n",
    "# 批量文本\n",
    "input_texts = [input_text, \"在此之前，越南、俄罗斯等国也都分别举行了抗战胜利纪念活动。\"]\n",
    "vectors = embed.embed_documents(input_texts)\n",
    "print(len(vectors))\n",
    "print(vectors[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ed50c",
   "metadata": {},
   "source": [
    "通过Embedding获取向量后，需要将向量存储起来，以便大模型检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    "    base_url = 'http://localhost:11434'\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "text1 = \"2024/2025赛季，利物浦以绝对优势占据了积分榜榜首，它将是这个赛季的英超冠军！\"\n",
    "text2 = \"根据2025年的国际形势，预测十年内出现重大军事冲突，或大范围的军事冲突可能行较大，但是发生世界大战的可能性较小。\"\n",
    "text3 = \"第23届世界杯将由2026年在加拿大、墨西哥、美国联合举办，是世界杯史上首次由3个国家联合举办同1届赛事。\"\n",
    "\n",
    "def test_local1():\n",
    "    '''\n",
    "    Embedding\n",
    "    '''\n",
    "    # 如果需要开启模型追踪，可以开启\n",
    "    # os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "    # os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "\n",
    "    \n",
    "    # 创建一个基于内存的向量存储\n",
    "    vectorstore = InMemoryVectorStore.from_texts(\n",
    "        [text1, text2, text3],\n",
    "        embedding = embeddings,\n",
    "    )\n",
    "    \n",
    "    #vectorstore.add_texts()\n",
    "    \n",
    "    # 使用向量存储作为检索器\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # 检索文本\n",
    "    #retrieved_documents = retriever.invoke(\"哪支俱乐部会是24/25赛季的英超冠军？\")\n",
    "    retrieved_documents = retriever.invoke(\"最近是否会发生世界大战？\")\n",
    "    # 获取检索到的内容，由于向量库内容较少，只能回答已有的知识\n",
    "    content = retrieved_documents[0].page_content\n",
    "    print(content)\n",
    "\n",
    "\n",
    "def test_local2():\n",
    "    query_result = embeddings.embed_query(text1)\n",
    "    print(\"文本向量长度：\", len(query_result), sep='')\n",
    "    \n",
    "    doc_results = embeddings.embed_documents(\n",
    "        [\n",
    "            \"第30轮，利物浦 vs 埃弗顿，比分：3:1\",\n",
    "            \"第31轮，富勒姆 vs 利物浦，比分：0:2\",\n",
    "            \"第32轮，利物浦 vs 西汉姆联，比分：2:1\",\n",
    "            \"第33轮，莱斯特城 vs 利物浦，比分：2:2\",\n",
    "            \"第34轮，利物浦 vs 热刺，比分：2:1\",\n",
    "            \"第35轮，切尔西 vs 利物浦，比分：0:1\",\n",
    "            \"第36轮，利物浦 vs 阿森纳，比分：2:1\",\n",
    "        ])\n",
    "    print(\"文本向量数量：\", len(doc_results), \"，文本向量长度：\", len(doc_results[0]), sep='')\n",
    "\n",
    "\n",
    "def test_local3():\n",
    "    document_1 = Document(id=\"1\", page_content=text1, metadata={\"冠军\": \"利物浦\"})\n",
    "    document_2 = Document(id=\"2\", page_content=text2, metadata={\"世界大战\": \"可能性较小\"})\n",
    "    document_3 = Document(id=\"3\", page_content=text3)\n",
    "    documents = [document_1, document_2, document_3]\n",
    "    \n",
    "    vectorstore = InMemoryVectorStore.from_texts(\n",
    "        [text1, text2, text3],\n",
    "        embedding = embeddings,\n",
    "    )\n",
    "    vectorstore.add_documents(documents=documents)\n",
    "    \n",
    "    top_n = 10\n",
    "    for index, (id, doc) in enumerate(vectorstore.store.items()):\n",
    "        if index < top_n:\n",
    "            # docs have keys 'id', 'vector', 'text', 'metadata'\n",
    "            print(f\"{id}: {doc['text']}\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # 删除\n",
    "    vectorstore.delete(ids=[\"3\"])\n",
    "    \n",
    "    # 搜索\n",
    "    results = vectorstore.similarity_search(query=\"冠军\",k=1)\n",
    "    for doc in results:\n",
    "        print(f\"* {doc.page_content} [{doc.metadata}]\")\n",
    "\n",
    "    # 解析器\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 1, \"fetch_k\": 2, \"lambda_mult\": 0.5},\n",
    "    )\n",
    "    result = retriever.invoke(\"冠军\")\n",
    "    print(result)\n",
    "\n",
    "\n",
    "test_local3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb409a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ded28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "query = 'apples'\n",
    "passages = [\n",
    "        'I like apples', \n",
    "        'I like oranges', \n",
    "        'Apples and oranges are fruits'\n",
    "    ]\n",
    "  \n",
    "# init embedding model\n",
    "model_name = 'maidalun1020/bce-embedding-base_v1'\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'batch_size': 64, 'normalize_embeddings': True, 'show_progress_bar': False}\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    "  )\n",
    "\n",
    "# example #1. extract embeddings\n",
    "query_embedding = embed_model.embed_query(query)\n",
    "passages_embeddings = embed_model.embed_documents(passages)\n",
    "\n",
    "# example #2. langchain retriever example\n",
    "faiss_vectorstore = FAISS.from_texts(passages, embed_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n",
    "\n",
    "retriever = faiss_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"score_threshold\": 0.5, \"k\": 3})\n",
    "\n",
    "related_passages = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(related_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e7ec2",
   "metadata": {},
   "source": [
    "参考：https://www.cnblogs.com/longronglang/p/18718487"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
