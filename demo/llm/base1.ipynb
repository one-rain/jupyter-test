{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876edee3",
   "metadata": {},
   "source": [
    "# 创建连接\n",
    "\n",
    "任何对话助手，都是先从创建一个模型服务连接开始。\n",
    "下面是未采用任何框架的连接方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa188f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 以本地ollama服务为例\n",
    "client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "# 使用模型提供商\n",
    "# client = OpenAI(api_key = \"你的api_key\", base_url = \"提供商的服务地址\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = 'qwen3', \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一名诗歌创作高手，尤其擅长中国古代诗歌。\"},\n",
    "        {\"role\": \"user\", \"content\": \"请创作一首跟端午有关的七言绝句。\"},\n",
    "    ])\n",
    "# print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615804f",
   "metadata": {},
   "source": [
    "下面是采用LangChain的连接方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031169ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        api_key = 'ollama',\n",
    "        base_url = 'http://localhost:11434/v1',\n",
    "        model = 'qwen3'\n",
    "        # other params...\n",
    "    )\n",
    "\n",
    "#或者\n",
    "\n",
    "llm = OllamaLLM(\n",
    "        base_url = 'http://localhost:11434', \n",
    "        model = 'qwen3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea64bb",
   "metadata": {},
   "source": [
    "基于 `ChatOllama` 或 `init_chat_model` 的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOllama(model = \"qwen3\",\n",
    "    base_url = 'http://localhost:11434', \n",
    "    temperature = 0.8,\n",
    "    num_predict = 256)\n",
    "# model = init_chat_model(\"gpt-4\", model_provider=\"openai\")\n",
    "\n",
    "messages = [\n",
    "        SystemMessage(content=\"你是一个乐于助人的助手，能做中英文互译。翻译用户句子。\"),\n",
    "        HumanMessage(content=\"我想成为一位超人！\")]\n",
    "\n",
    "# 第一种调用\n",
    "#response = llm.invoke(messages)\n",
    "#print(response)\n",
    "\n",
    "# 异步调用\n",
    "# await llm.ainvoke(messages)\n",
    "\n",
    "# 第二种，流式调用\n",
    "#for chunk in llm.stream(messages):\n",
    "#    print(chunk.text(), end=\"\")\n",
    "\n",
    "# 异步调用\n",
    "async for chunk in llm.astream(messages):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2cb31",
   "metadata": {},
   "source": [
    "如果想查看演示效果，可以使用Gradio。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb66966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat(message, history):\n",
    "    client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'qwen3', \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"你是一名诗歌创作高手，尤其擅长中国古代诗歌。\"},\n",
    "            {\"role\": \"user\", \"content\": \"请创作一首跟端午有关的七言绝句。\"},\n",
    "        ])\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()\n",
    "# 或者\n",
    "#interface = gr.Interface(fn=print_text, inputs=\"text\", outputs=\"text\")\n",
    "#interface.launch(share=True, server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df0496",
   "metadata": {},
   "source": [
    "# 流失输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    base_url = 'http://localhost:11434', \n",
    "    model = 'qwen3'\n",
    ")\n",
    "\n",
    "for chunk in llm.stream(\"海水为什么是蓝的？\"):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354f3ad",
   "metadata": {},
   "source": [
    "# 基于模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    base_url = 'http://localhost:11434', \n",
    "    model = 'qwen3'\n",
    ")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "    Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"question\": \"海水为什么是蓝的？\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd85d4",
   "metadata": {},
   "source": [
    "# chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    base_url = 'http://localhost:11434', \n",
    "    model = 'qwen3'\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are world class technical documentation writer.\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "question = \"how can langsmith help with testing?\"\n",
    "response = chain.invoke({\"input\": question})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
